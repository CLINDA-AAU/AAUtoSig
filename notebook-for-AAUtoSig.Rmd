---
title: "AAUtoSig first implementation"
author: "Ida Egendal"
date: "17 12 2021"
output: html_document
---

```{r, include=F}
library(reticulate)
matplotlib <- import("matplotlib", convert = TRUE)
matplotlib$use("Agg")
```


## Load packages
Firstly, we load the necessary packages for the code to run

```{python}
import torch
import torch.nn.functional as F

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

import copy

#because plots broke the kernel
import os
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"
```

## Load data
Secondly, we load the training and validation sets. These have been moved to the home directory, since we suspect that the connection to the Q-drive is what broke the Rstudio session, but the same files can also be found at Q:\AUH-HAEM-FORSK-MutSigDLBCL222\article_1\generated_data if someone is interested in repeating the analysis.

```{python}
mc = pd.read_csv(r'DLBCL1001_trainset1_80p.csv', sep=',', index_col=0).transpose()

context = mc.columns
mutation = [s[2:5] for s in context]

#split trainset into test and training
x_train = mc.sample(frac=0.8)
x_test = mc.drop(x_train.index)

x_train_tensor = torch.tensor(x_train.values, 
                              dtype = torch.float32)
x_test_tensor = torch.tensor(x_test.values, 
                             dtype = torch.float32)

trainloader = torch.utils.data.DataLoader(x_train_tensor, 
                                          batch_size=8, 
                                          shuffle=True)
                                          
validation_set = pd.read_csv(r'DLBCL1001_testset1_20p.csv', sep=',', index_col=0).transpose()
```

## Define Autoencoder
Pytorch is object oriented, which means that the active autoencoder will be an instance of the class defined in the chunk below.
This in an autoencoder with two encoding layers and one decoding layer, with the softplus activation function in each node.
Moreover the actual autoencoder is intialized with 30 nodes in the first encoding layer and 5 in the second, along with the mean squared error as a loss function and the adam optimizer.
```{python}
# Creating linear (NMF autoencoder)
# 96 ==> 8 ==> 96
class AAUtoSig(torch.nn.Module):
    def __init__(self, dim1, dim2):
        super().__init__()

        
        # Building an linear encoder
        # 96 => dim1 => dim2
        self.enc1 = torch.nn.Linear(96, dim1, bias = False)
        self.enc2 = torch.nn.Linear(dim1, dim2, bias = False)
          
        # Building an linear decoder 
        # dim ==> 96
        self.dec1 = torch.nn.Linear(dim2, 96, bias = False)
            

    def forward(self, x):
        x = F.softplus(self.enc1(x))
        x = F.softplus(self.enc2(x))
        x = F.softplus(self.dec1(x))
        return x
    
# Model Initialization
n_sigs = 5
model = AAUtoSig(dim1 = 30, dim2 = n_sigs)

# Validation using MSE Loss function
loss_function = torch.nn.MSELoss(reduction='mean')
#loss_function = torch.nn.KLDivLoss()

# Using an Adam Optimizer with lr = 0.1
optimizer = torch.optim.Adam(model.parameters(),
                             lr = 1e-3,
                             weight_decay = 1e-8)

```

## Training
Having initialized the autoencoder with the right dimensions (and other hyperparameters) training can begin.
For each epoc the model goes through a traing step and an evaluation step. Previously the evaluation step was used for patient early stopping, but as it interupted training when no actual overfitting was observed this has been removed. This also means that the test set is not really used for anything right now, other than making a plot.
```{python, message=F, results = 'hide'}
epochs = 500
outputs = []

training_plot=[]
test_plot=[]

last_score=np.inf
for epoch in range(epochs):
    model.train()
    
    for data in trainloader:
      # Output of Autoencoder
      reconstructed = model(data.view(-1,96))
        
      # Calculating the loss function
      loss = loss_function(reconstructed, data.view(-1,96))

      # The gradients are set to zero,
      # the the gradient is computed and stored.
      # .step() performs parameter update
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      #W = model.dec1.weight.data
    # print statistics
    with torch.no_grad():
        for p in model.parameters():
            p.clamp_(min = 0)
            
        model.eval()
        
        inputs = x_train_tensor[:]
        outputs = model(inputs)
        
        train_loss = loss_function(outputs, inputs)
        #train_loss = kl_poisson(inputs, outputs)

        training_plot.append(train_loss)
    
 

        inputs  = x_test_tensor[:]
        outputs = model(inputs)
        test_loss = loss_function(outputs, inputs)

        
        test_plot.append(test_loss)
        print("Epoch {}, training loss {}, test loss {}".format(epoch, 
                                                                      np.round(training_plot[-1],2), 
                                                                      np.round(test_plot[-1],2)))

 #save the model with lowest test loss
    if last_score > test_loss:
        last_score = test_loss
        best_model = copy.deepcopy(model)
```

## Visualizing the results
Having trained the model we visualize the train and test loss as a function of the epoch, and furthermore we visualize the found signatures using the function plotsigs:

```{python, message = F, results = 'hide'}
def plotsigs(context, mutation, intensities):
    colors = {'C>A': 'r', 'C>G': 'b', 'C>T': 'g', 
              'T>A' : 'y', 'T>C': 'c','T>G' : 'm' }
    plt.figure(figsize=(20,7))
    plt.bar(x = context, 
            height =  intensities/np.sum(intensities), 
            color = [colors[i] for i in mutation])
    labels = list(colors.keys())
    handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]
    plt.legend(handles,labels)
    plt.xticks(rotation=90)
    plt.show()
    
plt.figure(figsize=(16,12))
plt.subplot(3, 1, 1)
plt.title('Score per epoch')
plt.ylabel('Mean Squared Error')
plt.plot(list(range(len(training_plot))), test_plot, label='Validation MSE')
plt.plot(list(range(len(training_plot))), training_plot, label='Train MSE')
plt.legend()
plt.show()


W = best_model.dec1.weight.data    
W_array = W.numpy()


for i in range(n_sigs):
    plotsigs(context, mutation, W_array[:,i])    

```


## Validation error
Next, one can be interested in how well the found signatures/encoding layer generalizes. This is done by applying the model on the validation set, and calculating the out of sample error:

```{python}
x_validation_tensor = torch.tensor(validation_set.values, 
                                 dtype = torch.float32)
res = best_model(x_validation_tensor)
print(loss_function(res,x_validation_tensor))
```


